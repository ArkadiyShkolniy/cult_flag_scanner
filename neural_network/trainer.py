"""
–¢—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

from .model import FlagPatternCNN


class ModelTrainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    
    def __init__(self, model, device=None):
        """
        Args:
            model: –≠–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ FlagPatternCNN
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (cuda/cpu)
        """
        self.model = model
        
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
        
        self.model.to(self.device)
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'epochs': []
        }
    
    def train_epoch(self, train_loader, criterion, optimizer):
        """–û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in tqdm(train_loader, desc="Training"):
            images = images.to(self.device)
            labels = labels.to(self.device)
            
            # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
            optimizer.zero_grad()
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = self.model(images)
            loss = criterion(outputs, labels)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100 * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate(self, val_loader, criterion):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc="Validation"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(images)
                loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100 * correct / total
        
        return epoch_loss, epoch_acc
    
    def train(self, train_loader, val_loader=None, epochs=10, learning_rate=0.001,
              save_dir='models', save_best=True, save_last=True, resume_from=None):
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            save_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
            save_best: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            save_last: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª–∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
            resume_from: –ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        start_epoch = 0
        best_val_acc = 0.0
        
        if resume_from and os.path.exists(resume_from):
            checkpoint = torch.load(resume_from, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            start_epoch = checkpoint.get('epoch', 0) + 1
            best_val_acc = checkpoint.get('best_val_acc', 0.0)
            self.history = checkpoint.get('history', self.history)
            print(f"‚úÖ –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å —ç–ø–æ—Ö–∏ {start_epoch}")
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)
        
        print(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
        print(f"   –≠–ø–æ—Ö: {epochs}")
        print(f"   Learning rate: {learning_rate}")
        print(f"   –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_loader.dataset)}")
        
        for epoch in range(start_epoch, epochs):
            print(f"\n{'='*60}")
            print(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}")
            print(f"{'='*60}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_epoch(train_loader, criterion, optimizer)
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            if val_loader:
                val_loss, val_acc = self.validate(val_loader, criterion)
                self.history['val_loss'].append(val_loss)
                self.history['val_acc'].append(val_acc)
                self.history['epochs'].append(epoch + 1)
                
                print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º learning rate
                scheduler.step(val_loss)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
                if save_best and val_acc > best_val_acc:
                    best_val_acc = val_acc
                    best_model_path = os.path.join(save_dir, 'best_model.pth')
                    self.save_checkpoint(best_model_path, epoch, best_val_acc)
                    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (Val Acc: {val_acc:.2f}%)")
            else:
                self.history['epochs'].append(epoch + 1)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å
            if save_last:
                last_model_path = os.path.join(save_dir, 'last_model.pth')
                self.save_checkpoint(last_model_path, epoch, best_val_acc if val_loader else 0)
        
        print(f"\n{'='*60}")
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"{'='*60}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        history_path = os.path.join(save_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)
        print(f"üìä –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {history_path}")
    
    def save_checkpoint(self, path, epoch, best_val_acc):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç –º–æ–¥–µ–ª–∏"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'best_val_acc': best_val_acc,
            'history': self.history,
            'timestamp': datetime.now().isoformat()
        }
        torch.save(checkpoint, path)
    
    def fine_tune(self, new_data_loader, epochs=5, learning_rate=0.0001):
        """
        –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            new_data_loader: DataLoader —Å –Ω–æ–≤—ã–º–∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–æ–æ–±—É—á–µ–Ω–∏—è
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (–æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ —á–µ–º –ø—Ä–∏ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏)
        """
        print(f"üîÑ –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ {len(new_data_loader.dataset)} –Ω–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–∞—Ö...")
        
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        
        for epoch in range(epochs):
            print(f"\n–≠–ø–æ—Ö–∞ –¥–æ–æ–±—É—á–µ–Ω–∏—è {epoch + 1}/{epochs}")
            train_loss, train_acc = self.train_epoch(new_data_loader, criterion, optimizer)
            print(f"Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
    
    def predict_batch(self, data_loader):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ –¥–∞–Ω–Ω—ã—Ö
        
        Returns:
            predicted_labels, probabilities
        """
        self.model.eval()
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for images, _ in data_loader:
                images = images.to(self.device)
                preds, probs = self.model.predict(images)
                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        return np.array(all_preds), np.array(all_probs)


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫–∞
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫–∞...")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = FlagPatternCNN(num_classes=3)
    trainer = ModelTrainer(model)
    
    print(f"‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫ —Å–æ–∑–¥–∞–Ω –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {trainer.device}")

