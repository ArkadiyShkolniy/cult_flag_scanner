#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å keypoint detection –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ "–§–ª–∞–≥"
"""

import os
import sys
import argparse
from pathlib import Path
import torch
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

from neural_network.model_keypoints import create_keypoint_model
from neural_network.data_loader_keypoints import create_keypoint_data_loader
from neural_network.trainer_keypoints import KeypointModelTrainer


def main():
    parser = argparse.ArgumentParser(description='–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ —Å keypoint detection –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ "–§–ª–∞–≥"')
    parser.add_argument('--data_dir', type=str, default='neural_network/data',
                        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏ (default: neural_network/data)')
    parser.add_argument('--epochs', type=int, default=50,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö (default: 50)')
    parser.add_argument('--batch_size', type=int, default=8,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (default: 8)')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (default: 0.001)')
    parser.add_argument('--save_dir', type=str, default='neural_network/models',
                        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π (default: neural_network/models)')
    parser.add_argument('--image_size', type=int, default=224,
                        help='–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (default: 224)')
    parser.add_argument('--num_classes', type=int, default=3,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (default: 3: 0=–Ω–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–∞, 1=–±—ã—á–∏–π, 2=–º–µ–¥–≤–µ–∂–∏–π)')
    parser.add_argument('--num_keypoints', type=int, default=5,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (default: 5: T0-T4)')
    parser.add_argument('--train_split', type=float, default=0.8,
                        help='–î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (default: 0.8)')
    parser.add_argument('--classification_weight', type=float, default=1.0,
                        help='–í–µ—Å –¥–ª—è loss –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (default: 1.0)')
    parser.add_argument('--keypoint_weight', type=float, default=1.0,
                        help='–í–µ—Å –¥–ª—è loss —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫ (default: 1.0)')
    parser.add_argument('--order_penalty_weight', type=float, default=0.5,
                        help='–í–µ—Å –¥–ª—è penalty –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ —Ç–æ—á–µ–∫ T0<T1<T2<T3<T4 (default: 0.5, 0.0 = –æ—Ç–∫–ª—é—á–µ–Ω–æ)')
    parser.add_argument('--geometry_penalty_weight', type=float, default=0.5,
                        help='–í–µ—Å –¥–ª—è penalty –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∞–≤–∏–ª (default: 0.5, 0.0 = –æ—Ç–∫–ª—é—á–µ–Ω–æ)')
    parser.add_argument('--tolerance_normalized', type=float, default=0.003,
                        help='–ü–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö –¥–ª—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ (default: 0.003 = 0.3% –¥–ª—è 1h)')
    parser.add_argument('--window', type=int, default=100,
                        help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π –≤ –æ–∫–Ω–µ (default: 100)')
    parser.add_argument('--device', type=str, default='auto',
                        help='–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (cpu, cuda, –∏–ª–∏ auto)')
    parser.add_argument('--pretrained', type=str, default=None,
                        help='–ü—É—Ç—å –∫ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
    data_dir = Path(args.data_dir)
    annotations_file = data_dir / 'annotations.csv'
    
    if not annotations_file.exists():
        print(f"‚ùå –§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {annotations_file}")
        return
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print("=" * 60)
    print("üéì –û–ë–£–ß–ï–ù–ò–ï –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò –° KEYPOINT DETECTION")
    print("=" * 60)
    print()
    print(f"üìÅ –î–∞–Ω–Ω—ã–µ: {data_dir}")
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π: {save_dir}")
    print(f"‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"   ‚Ä¢ –≠–ø–æ—Ö: {args.epochs}")
    print(f"   ‚Ä¢ –ë–∞—Ç—á: {args.batch_size}")
    print(f"   ‚Ä¢ Learning rate: {args.learning_rate}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {args.image_size}x{args.image_size}")
    print(f"   ‚Ä¢ –ö–ª–∞—Å—Å–æ–≤: {args.num_classes}")
    print(f"   ‚Ä¢ –ö–ª—é—á–µ–≤—ã—Ö —Ç–æ—á–µ–∫: {args.num_keypoints}")
    print(f"   ‚Ä¢ –û–∫–Ω–æ —Å–≤–µ—á–µ–π: {args.window}")
    print(f"   ‚Ä¢ –í–µ—Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {args.classification_weight}")
    print(f"   ‚Ä¢ –í–µ—Å keypoints: {args.keypoint_weight}")
    print(f"   ‚Ä¢ –í–µ—Å order penalty: {args.order_penalty_weight} {'(–æ—Ç–∫–ª—é—á–µ–Ω–æ)' if args.order_penalty_weight == 0.0 else ''}")
    print(f"   ‚Ä¢ –í–µ—Å geometry penalty: {args.geometry_penalty_weight} {'(–æ—Ç–∫–ª—é—á–µ–Ω–æ)' if args.geometry_penalty_weight == 0.0 else ''}")
    print(f"   ‚Ä¢ –ü–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –¥–ª—è –≥–µ–æ–º–µ—Ç—Ä–∏–∏ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è): {args.tolerance_normalized} ({args.tolerance_normalized*100:.1f}%)")
    print(f"   ‚Ä¢ Train/Val split: {args.train_split:.1%}")
    print(f"   ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    try:
        train_loader, val_loader = create_keypoint_data_loader(
            str(data_dir),
            batch_size=args.batch_size,
            shuffle=True,
            image_size=(args.image_size, args.image_size),
            train_split=args.train_split,
            window=args.window
        )
        
        print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(train_loader.dataset)}")
        if val_loader is not None:
            print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(val_loader.dataset)}")
        else:
            print(f"   ‚ö†Ô∏è  –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –ø—É—Å—Ç (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–µ—Å—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train
        train_dataset = train_loader.dataset
        if hasattr(train_dataset, 'dataset'):
            # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è random_split
            actual_dataset = train_dataset.dataset
        else:
            actual_dataset = train_dataset
        
        labels = []
        for i in range(len(actual_dataset)):
            if i < len(train_loader.dataset):
                try:
                    _, label, _ = actual_dataset[i]
                    labels.append(label)
                except:
                    pass
        
        if labels:
            from collections import Counter
            label_counts = Counter(labels)
            print(f"   üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ–±—É—á–µ–Ω–∏–∏:")
            for label, count in sorted(label_counts.items()):
                class_name = {0: '–Ω–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–∞', 1: '–±—ã—á–∏–π —Ñ–ª–∞–≥', 2: '–º–µ–¥–≤–µ–∂–∏–π —Ñ–ª–∞–≥'}.get(label, f'–∫–ª–∞—Å—Å {label}')
                print(f"      ‚Ä¢ {class_name}: {count} –ø—Ä–∏–º–µ—Ä–æ–≤")
        print()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    print(f"üèóÔ∏è  –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_keypoint_model(
        num_classes=args.num_classes,
        num_keypoints=args.num_keypoints,
        image_height=args.image_size,
        image_width=args.image_size,
        pretrained_path=args.pretrained
    )
    model.to(device)
    print(f"   ‚úÖ –ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,} (–æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,})")
    print()
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
    
    # –¢—Ä–µ–Ω–∏—Ä–æ–≤—â–∏–∫
    trainer = KeypointModelTrainer(
        model=model,
        device=device,
        classification_weight=args.classification_weight,
        keypoint_weight=args.keypoint_weight,
        order_penalty_weight=args.order_penalty_weight,
        geometry_penalty_weight=args.geometry_penalty_weight,
        tolerance_normalized=args.tolerance_normalized
    )
    
    # –û–±—É—á–µ–Ω–∏–µ
    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    print()
    
    try:
        trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=args.epochs,
            optimizer=optimizer,
            scheduler=scheduler,
            save_dir=str(save_dir),
            save_prefix='keypoint_model'
        )
        
        print("=" * 60)
        print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print("=" * 60)
        print()
        print(f"üìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {save_dir}")
        print(f"   ‚Ä¢ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: keypoint_model_best.pth")
        print(f"   ‚Ä¢ –ü–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å: keypoint_model_last.pth")
        print()
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        print(f"üíæ –ü–æ—Å–ª–µ–¥–Ω—è—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_dir}/keypoint_model_last.pth")
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

