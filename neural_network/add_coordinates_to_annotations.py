#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Ç–æ—á–µ–∫ T0-T4 –≤ —É–∂–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

–í–∞—Ä–∏–∞–Ω—Ç—ã:
1. –ï—Å–ª–∏ –µ—Å—Ç—å JSON —Ñ–∞–π–ª—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Ö
2. –ï—Å–ª–∏ –Ω–µ—Ç - –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–∞–Ω–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
3. –ò–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–µ—Ä–µ—Ä–∞–∑–º–µ—Ç–∫—É —á–µ—Ä–µ–∑ labeling_dashboard
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

from neural_network.annotator import PatternAnnotator
from scanners.combined_scanner import ComplexFlagScanner
from dotenv import load_dotenv

load_dotenv()


def load_json_metadata(file_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except:
        return None


def find_pattern_with_scanner(df, label, scanner, timeframe='1h'):
    """
    –ü—ã—Ç–∞–µ—Ç—Å—è –Ω–∞–π—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω –≤ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞–Ω–µ—Ä–∞
    
    Args:
        df: DataFrame —Å–æ —Å–≤–µ—á–∞–º–∏
        label: –ú–µ—Ç–∫–∞ (1=–±—ã—á–∏–π, 2=–º–µ–¥–≤–µ–∂–∏–π)
        scanner: ComplexFlagScanner
        timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º
    
    Returns:
        dict —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ —Ç–æ—á–µ–∫ –∏–ª–∏ None
    """
    try:
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = scanner.analyze(df, timeframe=timeframe)
        
        if not patterns:
            return None
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–∏–ø—É (–±—ã—á–∏–π/–º–µ–¥–≤–µ–∂–∏–π)
        for pattern_info in patterns:
            is_bearish = "BEARISH" in pattern_info.get('pattern', '')
            pattern_label = 2 if is_bearish else 1
            
            if pattern_label == label:
                # –ù–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
                return {
                    'T0': pattern_info.get('t0'),
                    'T1': pattern_info.get('t1'),
                    'T2': pattern_info.get('t2'),
                    'T3': pattern_info.get('t3'),
                    'T4': pattern_info.get('t4')
                }
        
        return None
    except Exception as e:
        print(f"   ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–∞—Ç—Ç–µ—Ä–Ω–∞: {e}")
        return None


def add_coordinates_from_json(annotations_df, data_dir):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    data_dir = Path(data_dir)
    candles_dir = data_dir / 'candles'
    
    updated_count = 0
    
    for idx, row in annotations_df.iterrows():
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É–∂–µ –µ—Å—Ç—å
        if pd.notna(row.get('t0_idx')):
            continue
        
        file_path = data_dir / row['file']
        json_path = file_path.with_suffix('.json')
        
        if json_path.exists():
            metadata = load_json_metadata(json_path)
            if metadata and 't0' in metadata:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ JSON
                for point_name in ['T0', 'T1', 'T2', 'T3', 'T4']:
                    point_lower = point_name.lower()
                    point_data = metadata.get(point_lower)
                    
                    if point_data and isinstance(point_data, dict):
                        annotations_df.at[idx, f'{point_lower}_idx'] = point_data.get('idx')
                        annotations_df.at[idx, f'{point_lower}_price'] = point_data.get('price')
                
                updated_count += 1
                print(f"   ‚úÖ {row['file']}: –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã –∏–∑ JSON")
    
    return annotations_df, updated_count


def add_coordinates_from_scanner(annotations_df, data_dir, use_scanner=False):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏—Å–ø–æ–ª—å–∑—É—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–∞–Ω–µ—Ä (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    if not use_scanner:
        return annotations_df, 0
    
    token = os.environ.get("TINKOFF_INVEST_TOKEN")
    if not token:
        print("   ‚ö†Ô∏è  –¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∫–∞–Ω–µ—Ä")
        return annotations_df, 0
    
    scanner = ComplexFlagScanner(token)
    data_dir = Path(data_dir)
    updated_count = 0
    
    for idx, row in annotations_df.iterrows():
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —É–∂–µ –µ—Å—Ç—å
        if pd.notna(row.get('t0_idx')):
            continue
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º label=0 (–Ω–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω–∞)
        label = row.get('label', 0)
        if label == 0:
            continue
        
        file_path = data_dir / row['file']
        if not file_path.exists():
            continue
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ—á–∏
            df = pd.read_csv(file_path)
            timeframe = row.get('timeframe', '1h')
            
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
            points = find_pattern_with_scanner(df, label, scanner, timeframe)
            
            if points:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                for point_name in ['T0', 'T1', 'T2', 'T3', 'T4']:
                    point_lower = point_name.lower()
                    point_data = points.get(point_name)
                    
                    if point_data and isinstance(point_data, dict):
                        annotations_df.at[idx, f'{point_lower}_idx'] = point_data.get('idx')
                        annotations_df.at[idx, f'{point_lower}_price'] = point_data.get('price')
                
                updated_count += 1
                print(f"   ‚úÖ {row['file']}: –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞–π–¥–µ–Ω—ã —Å–∫–∞–Ω–µ—Ä–æ–º")
            else:
                print(f"   ‚ö†Ô∏è  {row['file']}: –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω —Å–∫–∞–Ω–µ—Ä–æ–º")
        
        except Exception as e:
            print(f"   ‚ùå {row['file']}: –æ—à–∏–±–∫–∞ - {e}")
    
    return annotations_df, updated_count


def main():
    import argparse
    parser = argparse.ArgumentParser(description='–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –≤ —É–∂–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã')
    parser.add_argument('--data_dir', type=str, default='neural_network/data',
                        help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--use_scanner', action='store_true',
                        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–∫–∞–Ω–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç')
    parser.add_argument('--dry_run', action='store_true',
                        help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å')
    
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir)
    annotations_file = data_dir / 'annotations.csv'
    
    if not annotations_file.exists():
        print(f"‚ùå –§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {annotations_file}")
        return
    
    print("=" * 60)
    print("üîß –î–û–ë–ê–í–õ–ï–ù–ò–ï –ö–û–û–†–î–ò–ù–ê–¢ –í –†–ê–ó–ú–ï–ß–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´")
    print("=" * 60)
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    annotations_df = pd.read_csv(annotations_file)
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(annotations_df)}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∏ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏
    coord_cols = [f'{p}_idx' for p in ['t0', 't1', 't2', 't3', 't4']]
    if not all(col in annotations_df.columns for col in coord_cols):
        print("   ‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∏ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –¥–æ–±–∞–≤–ª—è–µ–º...")
        for col in coord_cols + [col.replace('_idx', '_price') for col in coord_cols]:
            if col not in annotations_df.columns:
                annotations_df[col] = None
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    has_coords = annotations_df['t0_idx'].notna() if 't0_idx' in annotations_df.columns else pd.Series([False]*len(annotations_df))
    print(f"   ‚úÖ –° –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {has_coords.sum()}")
    print(f"   ‚ùå –ë–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {(~has_coords).sum()}")
    print()
    
    if has_coords.sum() == len(annotations_df):
        print("‚úÖ –í—Å–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —É–∂–µ –∏–º–µ—é—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã!")
        return
    
    # –ü–æ–ø—ã—Ç–∫–∞ 1: –î–æ–±–∞–≤–ª—è–µ–º –∏–∑ JSON —Ñ–∞–π–ª–æ–≤
    print("üìã –ü–æ–ø—ã—Ç–∫–∞ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏–∑ JSON —Ñ–∞–π–ª–æ–≤...")
    annotations_df, json_count = add_coordinates_from_json(annotations_df, data_dir)
    print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ JSON: {json_count}")
    print()
    
    # –ü–æ–ø—ã—Ç–∫–∞ 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–∞–Ω–µ—Ä (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω —Ñ–ª–∞–≥)
    if args.use_scanner:
        print("üìã –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–æ–∏—Å–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Å –ø–æ–º–æ—â—å—é –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–∫–∞–Ω–µ—Ä–∞...")
        annotations_df, scanner_count = add_coordinates_from_scanner(annotations_df, data_dir, use_scanner=True)
        print(f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–∫–∞–Ω–µ—Ä–æ–º: {scanner_count}")
        print()
    else:
        scanner_count = 0
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    has_coords_after = annotations_df['t0_idx'].notna()
    still_missing = (~has_coords_after).sum()
    
    print("=" * 60)
    print("üìä –ò–¢–û–ì–ò")
    print("=" * 60)
    print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ –∏–∑ JSON: {json_count}")
    print(f"   –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–∫–∞–Ω–µ—Ä–æ–º: {scanner_count}")
    print(f"   –í—Å–µ–≥–æ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏: {has_coords_after.sum()}")
    print(f"   –í—Å–µ –µ—â–µ –±–µ–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {still_missing}")
    print()
    
    if still_missing > 0:
        print("‚ö†Ô∏è  –î–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω—É–∂–Ω–æ:")
        print("   1. –ü–µ—Ä–µ—Ä–∞–∑–º–µ—Ç–∏—Ç—å —á–µ—Ä–µ–∑ labeling_dashboard")
        print("   2. –ò–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å JSON —Ñ–∞–π–ª—ã —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏")
        print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º (–µ—Å–ª–∏ –Ω–µ dry_run)
    if not args.dry_run:
        annotations_df.to_csv(annotations_file, index=False)
        print(f"‚úÖ –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {annotations_file}")
    else:
        print("üîç DRY RUN: –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–µ–∑ --dry_run –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è)")
    
    print("=" * 60)


if __name__ == "__main__":
    main()

